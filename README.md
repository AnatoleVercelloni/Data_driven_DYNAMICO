# Data_driven_DYNAMICO

This repository contains my work for my master thesis about data driven climate modelling based at the lsce
It contains all the steps to construct a emulator of the physics parametrizations of DYNAMICO and couple the emulator with the dynamical core of DYNAMICO
The emulator is trained from training data generated by ClimSIm, an open dataset (https://leap-stc.github.io/ClimSim/README.html)
The training data is assumed to be already downloaded (in $DSDIR on Jean-Zay)

This repository contains the following directories:


    (1) data_processing : scripts to put the data into a more suitable format than netcdf, which is .npy
        - check_preprocessing.ipynb    => notebook to check the validity of the procesed data
        - compute_norm_factor.py       => script to put the data into npy files and compute normalization factors
        - construct_dataset.py         => script to construct an array vizualizable by psyplot from npy or netcdf files
        - glob_factors.py              => script to reduce the normalization factors to global ones

    (2) NeuralNetworks  : scripts to define the architectures and train them
        - Dense.py                     => script to train a Dense neural network 

    (3) evaluation      : notebook to offline evaluate the emulators
        - construc_dataset.py          => script to be able to vizualize data on map
        - Dense_evaluation.ipynb       => notebook to do anoffline evaluation of the Dense Network

    (4) BuildGrid       : scripts to be able to create maps of the data
        - folder scvt                  => contains scripts written by Thomas Dubos for Delaunay triangulation and projection
        - construct_mesh.py            => contains python functions used by create_boundaries.py
        - create_boundaries.py         => to build the mesh
        - Example_Build_Grid.ipynb     => notebook to see how works the mesh construction and to construct it

    (5) RUNDIR          : scripts to run the different scripts listed before
        - Densejob                     => script to train the Dense neural network
        - prepro_job                   => script to preprocessed the data (not to be used directly)
        - processing.sh                => script to preprocessed the data (call prepro_job)

    (6) saved           : to saved models and normalization factors 
        - folder normalization_factors => folder to save the normalization factors
        - folder models                => folder to save the models


Also, we included a arch.env that contains the environment that has to be used (on Jean-Zay again)


_______How to use this repository ? (on Jean-Zaz again)____________

I. clone the repository

II. preprocessing of the data
#The idea is to get from the too many netcdf files few .npy files that contains the data and build a training set(train + val) and a scoring test
#Moreover, we can take only a subset of the data available

    > cd RUNDIR       
    #go to the RUNDIR directory

    #====================TRAINING DATASET====================================================================#
    > ./processing.sh first_dataset
    # create a dataset made of .npy files from netcdf raw files
    # dataset is saved into the $SCRATCH

    > python ../data_processing/glob_factors.py first_dataset 5038080
    # to reduce the normalization_factors computed over several nodes, usefull for training dataset only
    # the parameters name_dataset and n_samples are indicated in the output => prepro_x.out
    # the normalization_factors are saved into saved/normalization_factors/'resolution'/'dataset_name'

    #======================SCORING DATASET====================================================================#
    > ./processing.sh scoring_set
    # create a dataset made of .npy files from netcdf raw files
    # dataset is saved into the $SCRATCH

    #======================CUSTOM DATASET=======================================================================#
    > ./processing.sh 'custom_set'
    # default dataset parameters has to be set in preprocessing.sh (resolution, amount of files, nodes, ..)

III. training the emulator

    > cd RUNDIR
    #go to RUNDIR directory

    >./Densejob
    # run the training of the DenseNeural network
    # it saves the models at each epoch in saved/models/Dense
    # it also generates a png of the learning rate and the loss along the training

IV. offline evaluation
    # the offline evalulation is a notebook since it is more suitable for this task
    > open and run the notebook Dense_evaluation.ipynb in the folder evaluation

V. online evaluation
